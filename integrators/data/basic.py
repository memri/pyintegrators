# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/basic.ipynb (unless otherwise specified).

__all__ = ['read_file', 'read_json', 'write_json', 'write_jsonl', 'download_file', 'unzip', 'PYI_HOME', 'PYI_TESTDATA',
           'HOME_DIR', 'MEMRI_DIR', 'MODEL_DIR', 'MEMRI_S3', 'URL_REGEX', 'strip_html', 'replace_urls',
           'replace_emails', 'EMAIL', 'URL']

# Cell
from ..imports import *
from urllib.request import urlretrieve
import requests
from tqdm import tqdm
from fastai.data.external import download_url
from fastai.basics import progress_bar
import zipfile

# Cell
Path.ls = lambda x: list(x.iterdir())
PYI_HOME = Path.cwd().parent
PYI_TESTDATA = PYI_HOME / "test" / "data"
HOME_DIR = Path.home()
MEMRI_DIR = HOME_DIR / ".memri"
MODEL_DIR =  MEMRI_DIR / "models"
MEMRI_S3 = "https://memri.s3-eu-west-1.amazonaws.com"
MODEL_DIR.mkdir(parents=True, exist_ok=True)


def read_file(path):
    return open(path, "r").read()

def read_json(path):
    with open(path) as json_file:
        return json.load(json_file)

def write_json(obj, fname):
    with open(fname, 'w') as file_out:
        json.dump(obj , file_out)


def write_jsonl(objs, fname):
    with open(fname, 'w') as file_out:
        for o in objs:
            json.dump(o, file_out)
            file_out.write('\n')

def download_file(url, output_path):
    if not Path(output_path).exists():
        output_path.parent.mkdir(exist_ok=True, parents=True)
        print(f"downloading {url}", flush=True)

        # this may look overly complicated, but we want the system to first completely download and then write
        # , to prevent caching issues.
        response = requests.get(url, stream=True)
        total_length = response.headers.get('content-length')
        if total_length is None: # no content length header
            res = response.content
            with open(output_path, "wb") as f:
                f.write(res)
        else:
            dl = 0
            total_length = int(total_length)
            pbar = progress_bar(range(total_length), leave=False)
            pbar.update(0)
            res = b''
            for data in response.iter_content(chunk_size=1024*1024):
                dl += len(data)
                pbar.update(dl)
                res += data
            with open(output_path, "wb") as f:
                f.write(res)

def unzip(f, dest):
    with zipfile.ZipFile(str(f)) as zf:
        zf.extractall(str(dest))

# Cell
URL_REGEX = r'\b((?:https?://)?(?:(?:www\.)?(?:[\da-z\.-]+)\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\w\.-]*)*/?)\b'

# Cell
from bs4 import BeautifulSoup
EMAIL = "XX_EMAIL"
URL   = "XX_URL"

def strip_html(html):
    soup = BeautifulSoup(html, features="lxml")
    res = soup.get_text()
    return res

def replace_urls(s):
    return re.sub(URL_REGEX, EMAIL, s)

def replace_emails(s):
    return re.sub(r'[\w\.-]+@[\w\.-]+', URL, s)