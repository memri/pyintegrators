{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# default_exp indexers.faceclustering.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import ipdb\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from mmcv.runner import load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def confidence2clusters(confidences, dists, nbrs, tau):\n",
    "    pred_dist2peak, pred_peaks = confidence_to_peaks(dists, nbrs, confidences, max_conn=1)\n",
    "    cluster_labels = peaks_to_labels(pred_peaks, pred_dist2peak, tau=tau, inst_num=dists.shape[0])\n",
    "    return cluster_labels\n",
    "\n",
    "def group_clusters(photos, cluster_labels, min_cluster_size=2):\n",
    "    clusters = [c for c in set(cluster_labels) if list(cluster_labels).count(c) >= min_cluster_size]\n",
    "    return [np.array(photos)[np.where(cluster_labels == c)[0]] for c in clusters]\n",
    "\n",
    "def sparse_mx_to_indices_values(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    values = sparse_mx.data\n",
    "    shape = np.array(sparse_mx.shape)\n",
    "    return indices, values, shape\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    indices, values, shape = sparse_mx_to_indices_values(sparse_mx)\n",
    "    return indices_values_to_sparse_tensor(indices, values, shape)\n",
    "\n",
    "def indices_values_to_sparse_tensor(indices, values, shape):\n",
    "    indices = torch.from_numpy(indices)\n",
    "    values = torch.from_numpy(values)\n",
    "    shape = torch.Size(shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def build_symmetric_adj(adj, self_loop=True):\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    if self_loop:\n",
    "        adj = adj + sp.eye(adj.shape[0])\n",
    "    return adj\n",
    "\n",
    "def row_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    # if rowsum <= 0, keep its previous value\n",
    "    rowsum[rowsum <= 0] = 1\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def peaks_to_edges(peaks, dist2peak, tau):\n",
    "    edges = []\n",
    "    for src in peaks:\n",
    "        dsts = peaks[src]\n",
    "        dists = dist2peak[src]\n",
    "        for dst, dist in zip(dsts, dists):\n",
    "            if src == dst or dist >= 1 - tau:\n",
    "                continue\n",
    "            edges.append([src, dst])\n",
    "    return edges\n",
    "\n",
    "def confidence_to_peaks(dists, nbrs, confidence, max_conn=1):\n",
    "    # Note that dists has been sorted in ascending order\n",
    "    assert dists.shape[0] == confidence.shape[0]\n",
    "    assert dists.shape == nbrs.shape\n",
    "\n",
    "    num, _ = dists.shape\n",
    "    dist2peak = {i: [] for i in range(num)}\n",
    "    peaks = {i: [] for i in range(num)}\n",
    "\n",
    "    for i, nbr in enumerate(nbrs):\n",
    "        nbr_conf = confidence[nbr]\n",
    "        for j, c in enumerate(nbr_conf):\n",
    "            nbr_idx = nbr[j]\n",
    "            if i == nbr_idx or c <= confidence[i]:\n",
    "                continue\n",
    "            dist2peak[i].append(dists[i, j])\n",
    "            peaks[i].append(nbr_idx)\n",
    "            if len(dist2peak[i]) >= max_conn:\n",
    "                break\n",
    "    return dist2peak, peaks\n",
    "\n",
    "def _find_parent(parent, u):\n",
    "    idx = []\n",
    "    # parent is a fixed point\n",
    "    while (u != parent[u]):\n",
    "        idx.append(u)\n",
    "        u = parent[u]\n",
    "    for i in idx:\n",
    "        parent[i] = u\n",
    "    return u\n",
    "\n",
    "def peaks_to_labels(peaks, dist2peak, tau, inst_num):\n",
    "    edges = peaks_to_edges(peaks, dist2peak, tau)\n",
    "    pred_labels = edge_to_connected_graph(edges, inst_num)\n",
    "    return pred_labels\n",
    "\n",
    "def build_knns(knn_prefix, feats, knn_method, k,num_process=None, is_rebuild=False, feat_create_time=None):\n",
    "    knn_prefix = os.path.join(knn_prefix, '{}_k_{}'.format(knn_method, k))\n",
    "    index_path = knn_prefix + '.index'\n",
    "    index = knn_faiss(feats, k, index_path, omp_num_threads=num_process, rebuild_index=True)\n",
    "    knns = index.knns\n",
    "    return knns\n",
    "\n",
    "def edge_to_connected_graph(edges, num):\n",
    "    parent = list(range(num))\n",
    "    for u, v in edges:\n",
    "        p_u = _find_parent(parent, u)\n",
    "        p_v = _find_parent(parent, v)\n",
    "        parent[p_u] = p_v\n",
    "\n",
    "    for i in range(num):\n",
    "        parent[i] = _find_parent(parent, i)\n",
    "    remap = {}\n",
    "    uf = np.unique(np.array(parent))\n",
    "    for i, f in enumerate(uf):\n",
    "        remap[f] = i\n",
    "    cluster_id = np.array([remap[f] for f in parent])\n",
    "    return cluster_id\n",
    "\n",
    "def knns2ordered_nbrs(knns, sort=True):\n",
    "    if isinstance(knns, list):\n",
    "        knns = np.array(knns)\n",
    "    nbrs = knns[:, 0, :].astype(np.int32)\n",
    "    dists = knns[:, 1, :]\n",
    "    if sort:\n",
    "        # sort dists from low to high\n",
    "        nb_idx = np.argsort(dists, axis=1)\n",
    "        idxs = np.arange(nb_idx.shape[0]).reshape(-1, 1)\n",
    "        dists = dists[idxs, nb_idx]\n",
    "        nbrs = nbrs[idxs, nb_idx]\n",
    "    return dists, nbrs\n",
    "\n",
    "def read_meta(fn_meta, start_pos=0, verbose=True):\n",
    "    lb2idxs = {}\n",
    "    idx2lb = {}\n",
    "    with open(fn_meta) as f:\n",
    "        for idx, x in enumerate(f.readlines()[start_pos:]):\n",
    "            lb = int(x.strip())\n",
    "            if lb not in lb2idxs:\n",
    "                lb2idxs[lb] = []\n",
    "            lb2idxs[lb] += [idx]\n",
    "            idx2lb[idx] = lb\n",
    "\n",
    "    inst_num = len(idx2lb)\n",
    "    cls_num = len(lb2idxs)\n",
    "    if verbose:\n",
    "        print('[{}] #cls: {}, #inst: {}'.format(fn_meta, cls_num, inst_num))\n",
    "    return lb2idxs, idx2lb\n",
    "\n",
    "def read_probs(path, inst_num, feat_dim, dtype=np.float32, verbose=False):\n",
    "    assert (inst_num > 0 or inst_num == -1) and feat_dim > 0\n",
    "    count = -1\n",
    "    if inst_num > 0:\n",
    "        count = inst_num * feat_dim\n",
    "    probs = np.fromfile(path, dtype=dtype, count=count)\n",
    "\n",
    "    if feat_dim > 1:\n",
    "        probs = probs.reshape(inst_num, feat_dim)\n",
    "    if verbose:\n",
    "        print('[{}] shape: {}'.format(path, probs.shape))\n",
    "    return probs\n",
    "\n",
    "def fast_knns2spmat(knns, k, th_sim=0.7, use_sim=False, fill_value=None):\n",
    "    # convert knns to symmetric sparse matrix\n",
    "    from scipy.sparse import csr_matrix\n",
    "    eps = 1e-5\n",
    "    n = len(knns)\n",
    "    if isinstance(knns, list):\n",
    "        knns = np.array(knns)\n",
    "    nbrs = knns[:, 0, :]\n",
    "    dists = knns[:, 1, :]\n",
    "    # assert -eps <= dists.min() <= dists.max(\n",
    "    # ) <= 1 + eps, \"min: {}, max: {}\".format(dists.min(), dists.max())\n",
    "    if use_sim:\n",
    "        sims = 1. - dists\n",
    "    else:\n",
    "        sims = dists\n",
    "    if fill_value is not None:\n",
    "        print('[fast_knns2spmat] edge fill value:', fill_value)\n",
    "        sims.fill(fill_value)\n",
    "    row, col = np.where(sims >= th_sim)\n",
    "    # remove the self-loop\n",
    "    idxs = np.where(row != nbrs[row, col])\n",
    "    row = row[idxs]\n",
    "    col = col[idxs]\n",
    "    data = sims[row, col]\n",
    "    col = nbrs[row, col]  # convert to absolute column\n",
    "    assert len(row) == len(col) == len(data)\n",
    "    spmat = csr_matrix((data, (row, col)), shape=(n, n))\n",
    "    return spmat\n",
    "\n",
    "def l2norm(vec):\n",
    "    vec /= np.linalg.norm(vec, axis=1).reshape(-1, 1)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class knn_faiss():\n",
    "    def __init__(self, feats, k, index_key='', nprobe=128, omp_num_threads=None,\n",
    "                 rebuild_index=True, verbose=True,**kwargs):\n",
    "        import faiss\n",
    "        if omp_num_threads is not None:\n",
    "            faiss.omp_set_num_threads(omp_num_threads)\n",
    "        self.verbose = verbose\n",
    "        feats = feats.astype('float32')\n",
    "        _, dim = feats.shape\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        index.add(feats)\n",
    "        sims, nbrs = index.search(feats, k=k)\n",
    "        self.knns = [(np.array(nbr, dtype=np.int32), 1 - np.array(sim, dtype=np.float32))\n",
    "                     for nbr, sim in zip(nbrs, sims)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted basic.ipynb.\n",
      "Converted importers.EmailImporter.ipynb.\n",
      "Converted importers.Importer.ipynb.\n",
      "Converted importers.util.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted indexers.FaceClusteringIndexer.Models.ipynb.\n",
      "Converted indexers.FaceClusteringIndexer.Utils.ipynb.\n",
      "Converted indexers.FaceClusteringIndexer.indexer.ipynb.\n",
      "Converted indexers.FaceRecognitionModel.ipynb.\n",
      "Converted indexers.FacerecognitionIndexer.Photo.ipynb.\n",
      "Converted indexers.GeoIndexer.ipynb.\n",
      "Converted indexers.NoteListIndexer.NoteList.ipynb.\n",
      "Converted indexers.NoteListIndexer.Parser.ipynb.\n",
      "Converted indexers.NoteListIndexer.ipynb.\n",
      "Converted indexers.NoteListIndexer.util.ipynb.\n",
      "Converted indexers.indexer.ipynb.\n",
      "Converted itembase.ipynb.\n",
      "Converted pod.client.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
