{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp indexers.notelist.notelist_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pyintegrators.indexers.notelist.util import *\n",
    "from pyintegrators.indexers.notelist.notelist import *\n",
    "from pyintegrators.indexers.indexer import *\n",
    "from pyintegrators.indexers.notelist.parser import *\n",
    "from pyintegrators.data.schema import *\n",
    "from pyintegrators.data.basic import *\n",
    "from pyintegrators.imports import *\n",
    "from pyintegrators.pod.client import PodClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoteListIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import spacy \n",
    "# from imdb import  IMDb\n",
    "\n",
    "class NotesListIndexer(Indexer):\n",
    "    \"\"\"Extracts lists from notes and categorizes them.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def get_data(self, client):\n",
    "        notes  = [d.expand(client) for d in indexer_run.get_data(client)]\n",
    "        return notes\n",
    "\n",
    "    def index(self, client, indexer_run, notes):\n",
    "        \"\"\"Run indexer\"\"\"        \n",
    "        predictor = ListTypePredictor()\n",
    "        for note in notes:\n",
    "            parser = HTMLListParser()\n",
    "            lists = parser.get_lists(note)\n",
    "            for l in lists:\n",
    "                cat = l.infer_cat_from_title()\n",
    "                if cat is not None:\n",
    "                    l.category = cat\n",
    "                else:\n",
    "                    predictor.predict(l, assign=True)\n",
    "        \n",
    "        spans = [l.span for l in lists]\n",
    "\n",
    "        updates_nodes = notes\n",
    "        new_nodes = lists + spans\n",
    "\n",
    "        return updates_nodes, new_nodes\n",
    "\n",
    "class ListTypePredictor():\n",
    "    \"\"\"Predicts one of `LIST_CLASSES` for a list in a note.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = LIST_CLASSES\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "#         self.ia  = IMDb()\n",
    "        self.verb_like_tags = [\"VB\", \"VBP\"]\n",
    "        \n",
    "    \n",
    "    def predict(self, l, assign=False):\n",
    "        items = l.get_items(remove_html_=True)\n",
    "        preds = []\n",
    "        \n",
    "        for item in items:\n",
    "            preds.append(self.predict_item(item))\n",
    "        \n",
    "        pred = max(set(preds), key=preds.count)\n",
    "        \n",
    "        if assign: l.category = pred\n",
    "            \n",
    "        return pred\n",
    "            \n",
    "    def predict_item(self, item):        \n",
    "        if   self.is_movie(item):   return \"towatch\"\n",
    "        elif self.is_toread(item):  return \"toread\"\n",
    "        elif self.is_podcast(item): return \"tolisten\"\n",
    "        elif self.is_tobuy(item):   return \"tobuy\"\n",
    "        elif self.is_todo(item):    return \"todo\"\n",
    "        else:                       return \"uknown\"\n",
    "        \n",
    "\n",
    "    def is_todo(self, item):\n",
    "        doc = self.nlp(item)\n",
    "        if doc[0].tag_ in self.verb_like_tags: return True\n",
    "        else: return False\n",
    "        \n",
    "    \n",
    "    def is_movie(self, item):\n",
    "#         result = self.ia.search_movie(item)\n",
    "#         print(result)\n",
    "        return False\n",
    "    \n",
    "    def is_toread(self, item):\n",
    "        return False\n",
    "\n",
    "    def is_podcast(self, item):\n",
    "        return False\n",
    "\n",
    "    def is_tobuy(self, item):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_registration(NotesListIndexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = PodClient()\n",
    "\n",
    "evernote_file = PYI_TESTDATA / \"notes\" / \"evernote\" / \"evernote-test-note-1.html\"\n",
    "txt = read_file(evernote_file)\n",
    "note = INote.from_data(content=txt)\n",
    "\n",
    "indexer = NotesListIndexer.from_data()\n",
    "indexer_run = IndexerRun.from_data(progress=0, targetDataType=\"Note\")\n",
    "\n",
    "indexer_run.add_edge(\"indexer\", indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NotesListIndexer (#None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NotesListIndexer(client,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted basic.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted indexers.GeoIndexer.ipynb.\n",
      "Converted indexers.NoteListIndexer.ipynb.\n",
      "Converted indexers.indexer.ipynb.\n",
      "Converted itembase.ipynb.\n",
      "Converted pod.client.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
